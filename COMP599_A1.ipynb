{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobraine/Natural-Language-Understanding/blob/main/COMP599_A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "PViAoa7857Kz",
        "outputId": "77a1bf06-eba4-45c3-859f-e241bf99408a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-647a5c5f31da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m     ]\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'premise'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;31m# valid=tokenize(valid['premise'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-647a5c5f31da>\u001b[0m in \u001b[0;36mload_datasets\u001b[0;34m(data_directory)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train.json'"
          ]
        }
      ],
      "source": [
        "from typing import Union, Iterable, Callable\n",
        "import random\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "\n",
        "def load_datasets(data_directory: str) -> Union[dict, dict]:\n",
        "    \"\"\"\n",
        "    Reads the training and validation splits from disk and load\n",
        "    them into memory.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data_directory: str\n",
        "        The directory where the data is stored.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    train: dict\n",
        "        The train dictionary with keys 'premise', 'hypothesis', 'label'.\n",
        "    validation: dict\n",
        "        The validation dictionary with keys 'premise', 'hypothesis', 'label'.\n",
        "    \"\"\"\n",
        "    import json\n",
        "    import os\n",
        "\n",
        "    with open(os.path.join(data_directory, \"train.json\"), \"r\") as f:\n",
        "        train = json.load(f)\n",
        "\n",
        "    with open(os.path.join(data_directory, \"validation.json\"), \"r\") as f:\n",
        "        valid = json.load(f)\n",
        "\n",
        "    return train, valid\n",
        "\n",
        "\n",
        "def tokenize(\n",
        "    text: \"list[str]\", max_length: int = None, normalize: bool = True\n",
        ") -> \"list[list[str]]\":\n",
        "    \"\"\"\n",
        "    Tokenize the text into individual words (nested list of string),\n",
        "    where the inner list represent a single example.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text: list of strings\n",
        "        Your cleaned text data (either premise or hypothesis).\n",
        "    max_length: int, optional\n",
        "        The maximum length of the sequence. If None, it will be\n",
        "        the maximum length of the dataset.\n",
        "    normalize: bool, default True\n",
        "        Whether to normalize the text before tokenizing (i.e. lower\n",
        "        case, remove punctuations)\n",
        "    Returns\n",
        "    -------\n",
        "    list of list of strings\n",
        "        The same text data, but tokenized by space.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> tokenize(['Hello, world!', 'This is a test.'], normalize=True)\n",
        "    [['hello', 'world'], ['this', 'is', 'a', 'test']]\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    if normalize:\n",
        "        regexp = re.compile(\"[^a-zA-Z ]+\")\n",
        "        # Lowercase, Remove non-alphanum\n",
        "        text = [regexp.sub(\"\", t.lower()) for t in text]\n",
        "\n",
        "    return [t.split()[:max_length] for t in text]\n",
        "\n",
        "\n",
        "def build_word_counts(token_list: \"list[list[str]]\") -> \"dict[str, int]\":\n",
        "    \"\"\"\n",
        "    This builds a dictionary that keeps track of how often each word appears\n",
        "    in the dataset.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    token_list: list of list of strings\n",
        "        The list of tokens obtained from tokenize().\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict of {str: int}\n",
        "        A dictionary mapping every word to an integer representing the\n",
        "        appearance frequency.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    If you have  multiple lists, you should concatenate them before using\n",
        "    this function, e.g. generate_mapping(list1 + list2 + list3)\n",
        "    \"\"\"\n",
        "    word_counts = {}\n",
        "\n",
        "    for words in token_list:\n",
        "        for word in words:\n",
        "            word_counts[word] = word_counts.get(word, 0) + 1\n",
        "\n",
        "    return word_counts\n",
        "\n",
        "\n",
        "def build_index_map(\n",
        "    word_counts: \"dict[str, int]\", max_words: int = None\n",
        ") -> \"dict[str, int]\":\n",
        "    \"\"\"\n",
        "    Builds an index map that converts a word into an integer that can be\n",
        "    accepted by our model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    word_counts: dict of {str: int}\n",
        "        A dictionary mapping every word to an integer representing the\n",
        "        appearance frequency.\n",
        "    max_words: int, optional\n",
        "        The maximum number of words to be included in the index map. By\n",
        "        default, it is None, which means all words are taken into account.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict of {str: int}\n",
        "        A dictionary mapping every word to an integer representing the\n",
        "        index in the embedding.\n",
        "    \"\"\"\n",
        "\n",
        "    return {\n",
        "        word: ix\n",
        "        for ix, (word, _) in enumerate(\n",
        "            sorted(word_counts.items(), key=lambda item: item[1], reverse=True)[\n",
        "                :max_words\n",
        "            ]\n",
        "        )\n",
        "    }\n",
        "\n",
        "\n",
        "def tokens_to_ix(\n",
        "    tokens: \"list[list[str]]\", index_map: \"dict[str, int]\"\n",
        ") -> \"list[list[int]]\":\n",
        "    \"\"\"\n",
        "    Converts a nested list of tokens to a nested list of indices using\n",
        "    the index map.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    tokens: list of list of strings\n",
        "        The list of tokens obtained from tokenize().\n",
        "    index_map: dict of {str: int}\n",
        "        The index map from build_index_map().\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list of list of int\n",
        "        The same tokens, but converted into indices.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    Words that have not been seen are ignored.\n",
        "    \"\"\"\n",
        "    return [\n",
        "        [index_map[word] for word in words if word in index_map] for words in tokens\n",
        "    ]\n",
        "\n",
        "train, valid=load_datasets(\"/content\")\n",
        "print(valid['premise'])\n",
        "# valid=tokenize(valid['premise'])\n",
        "# print(valid)\n",
        "# valid_counts=build_word_counts(valid)\n",
        "# print(valid_counts)\n",
        "# valid_index=build_index_map(valid_counts)\n",
        "# print(valid_index)\n",
        "# tokens_final=tokens_to_ix(valid, valid_index)\n",
        "# print(tokens_final)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 1.1 Batching, shuffling, iteration\n",
        "def build_loader(\n",
        "    data_dict: dict, batch_size: int = 64, shuffle: bool = False\n",
        ") -> Callable[[], Iterable[dict]]:\n",
        "    \"\"\"\n",
        "    Build a nested function. build_loader(...) specifies what type of loader\n",
        "    you want, and the output is itself a function that, when called, returns\n",
        "    a generator. You can iterate over the generator to get a batch of data\n",
        "    (which is  a dictionary with the same keys).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data_dict: dict\n",
        "        A dictionary with keys 'premise', 'hypothesis', and potentially\n",
        "        'label', all of which are lists of same length.\n",
        "    batch_size: int, optional\n",
        "        The size of the batch.\n",
        "    shuffle: bool, optional\n",
        "        Whether to shuffle the dataset.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    function\n",
        "        A loader function with no input and returns an iterator yielding a\n",
        "        dictionary with the same keys as data_dict, but with length\n",
        "        corresponding to batch_size.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    It's possible to implement this function such that data_dict could have\n",
        "    arbitrary keys as long as they are all lists of same length.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> loader = build_loader(data)\n",
        "    >>> for batch in loader():\n",
        "    ...     premise = batch['premise']\n",
        "    ...     label_batch = batch['label']\n",
        "    ...     # do something with batch here\n",
        "    \"\"\"\n",
        "    # TODO: Your code here\n",
        "\n",
        "    def loader():\n",
        "        # TODO: Your code here\n",
        "        pass\n",
        "\n",
        "    return loader\n",
        "\n",
        "\n",
        "### 1.2 Converting a batch into inputs\n",
        "def convert_to_tensors(text_indices: \"list[list[str]]\") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Given a list of lists of indices, convert it to a tensor of shape (N, L).\n",
        "    You will need to handle the padding, which will be the integer\n",
        "    \"\"\"\n",
        "    # TODO: Your code here\n",
        "    pass\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uJakwpxV7Diq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 2.1 Design a logistic model with embedding and pooling\n",
        "def max_pool(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Take the pooling over the second dimension, i.e. a\n",
        "    (N, L, D) -> (N, D) transformation where D is the `hidden_size`,\n",
        "    N is the batch size, L is the sequence length.\n",
        "    \"\"\"\n",
        "    # TODO: Your code here\n",
        "    pass\n",
        "\n",
        "\n",
        "class PooledLogisticRegression(nn.Module):\n",
        "    def __init__(self, embedding: nn.Embedding):\n",
        "        \"\"\"\n",
        "        When called this simple linear model will do the following:\n",
        "            1. Individually embed a batch of premise and hypothesis (token indices)\n",
        "            2. Individually apply max_pool along the sequence length (L_p and L_h)\n",
        "            3. Concatenate the pooled tensors into a single tensor\n",
        "            4. Apply the logistic regression to obtain prediction\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        embedding: nn.Embedding\n",
        "            The embedding layer you created using the size of the word index.\n",
        "            You can create it outside of this module. The transformation is\n",
        "            (N, L) -> (N, L, E) where E is the initial embedding dimension, and L is\n",
        "            the sequence length.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: Your code here\n",
        "\n",
        "    # DO NOT CHANGE THE SECTION BELOW! ###########################\n",
        "    # # This is to force you to initialize certain things in __init__\n",
        "    def get_layer_pred(self):\n",
        "        return self.layer_pred\n",
        "\n",
        "    def get_embedding(self):\n",
        "        return self.embedding\n",
        "\n",
        "    def get_sigmoid(self):\n",
        "        return self.sigmoid\n",
        "\n",
        "    # DO NOT CHANGE THE SECTION ABOVE! ###########################\n",
        "\n",
        "    def forward(self, premise: torch.Tensor, hypothesis: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        premise: torch.Tensor[N, L_p]\n",
        "            The premise tensor, where L_p is the premise sequence length and\n",
        "            N is the batch size.\n",
        "        hypothesis: torch.Tensor[N, L_h]\n",
        "            The hypothesis tensor, where L_h is the hypothesis sequence length.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor[N]\n",
        "            The predicted score for each example in the batch.\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        Note the returned tensor is of shape N, not (N, 1). You will need to\n",
        "        reshape your tensor to get the correct format.\n",
        "        \"\"\"\n",
        "\n",
        "        emb = self.get_embedding()\n",
        "        layer_pred = self.get_layer_pred()\n",
        "        sigmoid = self.get_sigmoid()\n",
        "\n",
        "        # TODO: Your code here\n",
        "\n",
        "\n",
        "### 2.2 Choose an optimizer and a loss function\n",
        "def assign_optimizer(model: nn.Module, **kwargs) -> torch.optim.Optimizer:\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: nn.Module\n",
        "        The model to optimize.\n",
        "    kwargs: dict\n",
        "        The arguments to pass to the optimizer. This will vary depending on the\n",
        "        optimizer, but the most common one is `lr`.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.optim.Optimizer\n",
        "        The optimizer that you will use during the model training.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    There's many optimizers in PyTorch. You can start with SGD, but\n",
        "    it's recommended to try other popular options:\n",
        "    https://pytorch.org/docs/stable/optim.html#algorithms\n",
        "    \"\"\"\n",
        "    # TODO: Your code here\n",
        "    pass\n",
        "\n",
        "\n",
        "def bce_loss(y: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    The binary cross entropy loss, implemented from scratch using torch\n",
        "    (do not use torch.nn).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y: torch.Tensor[N]\n",
        "        The true labels.\n",
        "    y_pred: torch.Tensor[N]\n",
        "        The predicted labels.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        The binary cross entropy loss (averaged over N).\n",
        "    \"\"\"\n",
        "    # TODO: Your code here\n",
        "    pass\n",
        "\n",
        "\n",
        "### 2.3 Forward and backward pass\n",
        "def forward_pass(model: nn.Module, batch: dict, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Implement a function that performs one step of the training process. Given\n",
        "    a batch and a model, this function should handle the text to tensor conversion\n",
        "    and pass it in a model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: nn.Module\n",
        "        The model you will use to perform the forward pass.\n",
        "    batch: dict of list\n",
        "        A dictionary with 'premise' and 'hypothesis' keys (lists of same size).\n",
        "    device: str\n",
        "        The device you want to run the model on. This is usually 'cpu' or 'cuda'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        The predicted labels.\n",
        "\n",
        "    This function should return the predicted y value by the model.\n",
        "    \"\"\"\n",
        "    # TODO: Your code here\n",
        "    pass\n",
        "\n",
        "\n",
        "def backward_pass(\n",
        "    optimizer: torch.optim.Optimizer, y: torch.Tensor, y_pred: torch.Tensor\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    This function takes in the optimizer, the true labels, and the predicted labels,\n",
        "    then computes the loss and performs a backward pass before updating the weights.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    optimizer: torch.optim.Optimizer\n",
        "        The optimizer you will use to perform the backward pass.\n",
        "    y: torch.Tensor[N]\n",
        "        The true labels.\n",
        "    y_pred: torch.Tensor[N]\n",
        "        The predicted labels.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        The loss value computed with bce_loss()\n",
        "    \"\"\"\n",
        "    # TODO: Your code here\n",
        "    pass\n",
        "\n",
        "\n",
        "### 2.4 Evaluation\n",
        "def f1_score(y: torch.Tensor, y_pred: torch.Tensor, threshold=0.5) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute the F1 score from scratch (without using external libraries).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y: torch.Tensor[N]\n",
        "        The true labels.\n",
        "    y_pred: torch.Tensor[N]\n",
        "        The predicted labels.\n",
        "    threshold: float, default 0.5\n",
        "        The threshold to use to convert the predicted labels to binary. If set\n",
        "        to None, y_pred will not be thresholded.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor[1]\n",
        "        The F1 score.\n",
        "\n",
        "    \"\"\"\n",
        "    # TODO: Your code here\n",
        "    pass\n",
        "\n",
        "\n",
        "### 2.5 Train loop\n",
        "def eval_run(\n",
        "    model: nn.Module, loader: Callable[[], Iterable[dict]], device: str = \"cpu\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Iterate through a loader and predict the labels for each example, all while\n",
        "    collecting the original labels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: nn.Module\n",
        "        The model you will use to perform the forward pass.\n",
        "    loader: Callable[[], Iterable[dict]]\n",
        "        The loader function that will yield batches.\n",
        "    device: str\n",
        "        The device you want to run the model on. This is usually 'cpu' or 'cuda'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    y_true: torch.Tensor[N]\n",
        "        The true labels, extracted from the loader.\n",
        "    y_pred: torch.Tensor[N]\n",
        "        The labels predicted by the model (output of forward_pass).\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    You can use the `forward_pass` function to get the predicted labels. Don't\n",
        "    forget to  disable the gradients for the model and to turn your model into\n",
        "    evaluation mode.\n",
        "    \"\"\"\n",
        "    # TODO: Your code here\n",
        "    pass\n",
        "\n",
        "\n",
        "def train_loop(\n",
        "    model: nn.Module,\n",
        "    train_loader,\n",
        "    valid_loader,\n",
        "    optimizer,\n",
        "    n_epochs: int = 3,\n",
        "    device: str = \"cpu\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Train a model for a given number of epochs.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: nn.Module\n",
        "        The model you will use to perform the forward pass.\n",
        "    train_loader: Callable[[], Iterable[dict]]\n",
        "        The loader function that will yield shuffled batches of training data.\n",
        "    valid_loader: Callable[[], Iterable[dict]]\n",
        "        The loader function that will yield non-shuffled batches of validation data.\n",
        "    optimizer: torch.optim.Optimizer\n",
        "        The optimizer you will use to perform the backward pass.\n",
        "    n_epochs: int\n",
        "        The number of epochs you want to train your model\n",
        "    device: str\n",
        "        The device you want to run the model on. This is usually 'cpu' or 'cuda'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        A list of f1 scores evaluated on the valid_loader at the end of each epoch.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    This function is left open-ended and is strictly to help you train your model.\n",
        "    You are free to implement what you think works best, as long as it runs on the\n",
        "    training and validation data and return a list of validation score at the end\n",
        "    of each epoch.\n",
        "    \"\"\"\n",
        "    # TODO: Your code here\n",
        "    pass\n",
        "\n"
      ],
      "metadata": {
        "id": "Mcgl5u9aEMlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 3.1\n",
        "class ShallowNeuralNetwork(nn.Module):\n",
        "    def __init__(self, embedding: nn.Embedding, hidden_size: int):\n",
        "        \"\"\"\n",
        "        When called this simple linear model will do the following:\n",
        "            1. Individually embed a batch of premise and hypothesis (token indices)\n",
        "            2. Individually apply max_pool along the sequence length (L_p and L_h)\n",
        "            3. Individually apply one feedforward layer to your pooled tensors\n",
        "            4. Use the ReLU on the outputs of your layer\n",
        "            5. Concatenate the activated tensors into a single tensor\n",
        "            6. Apply sigmoid layer to obtain prediction\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        embedding: nn.Embedding\n",
        "            The embedding layer you created using the size of the word index.\n",
        "        hidden_size: int\n",
        "            The size of the hidden layer.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: continue here\n",
        "\n",
        "    # DO NOT CHANGE THE SECTION BELOW! ###########################\n",
        "    # # This is to force you to initialize certain things in __init__\n",
        "    def get_ff_layer(self):\n",
        "        return self.ff_layer\n",
        "\n",
        "    def get_layer_pred(self):\n",
        "        return self.layer_pred\n",
        "\n",
        "    def get_embedding(self):\n",
        "        return self.embedding\n",
        "\n",
        "    def get_sigmoid(self):\n",
        "        return self.sigmoid\n",
        "\n",
        "    def get_activation(self):\n",
        "        return self.activation\n",
        "\n",
        "    # DO NOT CHANGE THE SECTION ABOVE! ###########################\n",
        "\n",
        "    def forward(self, premise: torch.Tensor, hypothesis: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        premise: torch.Tensor[N, L_p]\n",
        "            The premise tensor, where N is the batch size and L_p is the premise\n",
        "            sequence length.\n",
        "        hypothesis: torch.Tensor[N, L_h]\n",
        "            The hypothesis tensor, where L_h is the hypothesis sequence length.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor[N]\n",
        "            The scores for each example in the batch.\n",
        "        \"\"\"\n",
        "\n",
        "        emb = self.get_embedding()\n",
        "        layer_pred = self.get_layer_pred()\n",
        "        sigmoid = self.get_sigmoid()\n",
        "        ff_layer = self.get_ff_layer()\n",
        "        act = self.get_activation()\n",
        "\n",
        "        # TODO: continue here\n",
        "\n",
        "\n",
        "### 3.2\n",
        "class DeepNeuralNetwork(nn.Module):\n",
        "    def __init__(self, embedding: nn.Embedding, hidden_size: int, num_layers: int = 2):\n",
        "        \"\"\"\n",
        "        When called this simple linear model will do the following:\n",
        "            1. Individually embed a batch of premise and hypothesis (token indices)\n",
        "            2. Individually apply max_pool along the sequence length (L_p and L_h)\n",
        "            3. Individually apply one feedforward layer to your pooled tensors\n",
        "            4. Use the ReLU on the outputs of your layer, repeat (3) for `num_layers` times.\n",
        "            5. Concatenate the activated tensors into a single tensor\n",
        "            6. Apply sigmoid layer to obtain prediction\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        embedding: nn.Embedding\n",
        "            The embedding layer you created using the size of the of the word index. You can\n",
        "            create it outside of this module. The transforma dimensions is (N, L) -> (N, L, E) where\n",
        "            E is the initial embedding dimension, and L is the sequence length.\n",
        "        hidden_size: int\n",
        "            The size of the hidden layer.\n",
        "        num_layers: int, default 2\n",
        "            The number of hidden layers in your deep network. Each layer must\n",
        "            be activated with ReLU.\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        You will need to use nn.ModuleList to track your layers.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: continue here\n",
        "\n",
        "    # DO NOT CHANGE THE SECTION BELOW! ###########################\n",
        "    # # This is to force you to initialize certain things in __init__\n",
        "    def get_ff_layers(self):\n",
        "        return self.ff_layers\n",
        "\n",
        "    def get_layer_pred(self):\n",
        "        return self.layer_pred\n",
        "\n",
        "    def get_embedding(self):\n",
        "        return self.embedding\n",
        "\n",
        "    def get_sigmoid(self):\n",
        "        return self.sigmoid\n",
        "\n",
        "    def get_activation(self):\n",
        "        return self.activation\n",
        "\n",
        "    # DO NOT CHANGE THE SECTION ABOVE! ###########################\n",
        "\n",
        "    def forward(self, premise: torch.Tensor, hypothesis: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        premise: torch.Tensor[N, L_p]\n",
        "            The premise tensor, where N is the batch size and L_p is the premise\n",
        "            sequence length.\n",
        "        hypothesis: torch.Tensor[N, L_h]\n",
        "            The hypothesis tensor, where L_h is the hypothesis sequence length.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor[N]\n",
        "            The scores for each example in the batch.\n",
        "        \"\"\"\n",
        "\n",
        "        emb = self.get_embedding()\n",
        "        layer_pred = self.get_layer_pred()\n",
        "        sigmoid = self.get_sigmoid()\n",
        "        ff_layers = self.get_ff_layers()\n",
        "        act = self.get_activation()\n",
        "\n",
        "        # TODO: continue here\n"
      ],
      "metadata": {
        "id": "3vpyde69EQsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # If you have any code to test or train your model, do it BELOW!\n",
        "\n",
        "    # Seeds to ensure reproducibility\n",
        "    random.seed(2022)\n",
        "    torch.manual_seed(2022)\n",
        "\n",
        "    # If you use GPUs, use the code below:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Prefilled code showing you how to use the helper functions\n",
        "    train_raw, valid_raw = load_datasets(\"data\")\n",
        "\n",
        "    train_tokens = {\n",
        "        \"premise\": tokenize(train_raw[\"premise\"], max_length=64),\n",
        "        \"hypothesis\": tokenize(train_raw[\"hypothesis\"], max_length=64),\n",
        "    }\n",
        "\n",
        "    valid_tokens = {\n",
        "        \"premise\": tokenize(valid_raw[\"premise\"], max_length=64),\n",
        "        \"hypothesis\": tokenize(valid_raw[\"hypothesis\"], max_length=64),\n",
        "    }\n",
        "\n",
        "    word_counts = build_word_counts(\n",
        "        train_tokens[\"premise\"]\n",
        "        + train_tokens[\"hypothesis\"]\n",
        "        + valid_tokens[\"premise\"]\n",
        "        + valid_tokens[\"hypothesis\"]\n",
        "    )\n",
        "    index_map = build_index_map(word_counts, max_words=10000)\n",
        "\n",
        "    train_indices = {\n",
        "        \"label\": train_raw[\"label\"],\n",
        "        \"premise\": tokens_to_ix(train_tokens[\"premise\"], index_map),\n",
        "        \"hypothesis\": tokens_to_ix(train_tokens[\"hypothesis\"], index_map)\n",
        "    }\n",
        "\n",
        "    valid_indices = {\n",
        "        \"label\": valid_raw[\"label\"],\n",
        "        \"premise\": tokens_to_ix(valid_tokens[\"premise\"], index_map),\n",
        "        \"hypothesis\": tokens_to_ix(valid_tokens[\"hypothesis\"], index_map)\n",
        "    }\n",
        "\n",
        "    # 1.1\n",
        "    train_loader = \"your code here\"\n",
        "    valid_loader = \"your code here\"\n",
        "\n",
        "    # 1.2\n",
        "    batch = next(train_loader())\n",
        "    y = \"your code here\"\n",
        "\n",
        "    # 2.1\n",
        "    embedding = \"your code here\"\n",
        "    model = \"your code here\"\n",
        "\n",
        "    # 2.2\n",
        "    optimizer = \"your code here\"\n",
        "\n",
        "    # 2.3\n",
        "    y_pred = \"your code here\"\n",
        "    loss = \"your code here\"\n",
        "\n",
        "    # 2.4\n",
        "    score = \"your code here\"\n",
        "\n",
        "    # 2.5\n",
        "    n_epochs = 2\n",
        "\n",
        "    embedding = \"your code here\"\n",
        "    model = \"your code here\"\n",
        "    optimizer = \"your code here\"\n",
        "\n",
        "    scores = \"your code here\"\n",
        "\n",
        "    # 3.1\n",
        "    embedding = \"your code here\"\n",
        "    model = \"your code here\"\n",
        "    optimizer = \"your code here\"\n",
        "\n",
        "    scores = \"your code here\"\n",
        "\n",
        "    # 3.2\n",
        "    embedding = \"your code here\"\n",
        "    model = \"your code here\"\n",
        "    optimizer = \"your code here\"\n",
        "\n",
        "    scores = \"your code here\""
      ],
      "metadata": {
        "id": "3qtdnH3HEUjR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}